{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/rom1504/img2dataset 大规模图文数据集读取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以COCO为例\n",
    "import os \n",
    "root_dir = '/home/chenweifeng/dataset/mm_data/COCO/mscoco_dataset/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "pq_files = [os.path.join(root_dir, f) for f in  os.listdir(root_dir) if f.endswith('.parquet')]\n",
    "print(pq_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = []\n",
    "for pq_file in pq_files:\n",
    "    table = pq.read_table(pq_file)\n",
    "    tables.append(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tables[0].column_names)\n",
    "print(tables[0]['caption'])\n",
    "print(tables[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webdataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from torchvision import transforms\n",
    "import webdataset as wds\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# url = \"/home/chenweifeng/dataset/mm_data/COCO/mscoco_dataset/images/{00000..00056}.tar\"\n",
    "url = \"/home/chenweifeng/dataset/mm_data/COCO/mscoco_dataset/images/{00000..00056}.tar\"\n",
    "\n",
    "\n",
    "dataset = wds.WebDataset(url)\n",
    "for sample in islice(dataset, 0, 2):\n",
    "    for key, value in sample.items():\n",
    "        print(key, repr(value)[:50])\n",
    "    print()\n",
    "dataset = (\n",
    "    wds.WebDataset(url)\n",
    "    .shuffle(1000)\n",
    "    .decode(\"rgb\")\n",
    "    .to_tuple(\"jpg;png\", \"json\")\n",
    ")\n",
    "\n",
    "# len(dataset)\n",
    "for image, data in islice(dataset, 0, 1):\n",
    "    print(image.shape, image.dtype, type(data), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "dataloader = torch.utils.data.DataLoader(dataset.batched(batch_size), num_workers=4, batch_size=None)\n",
    "# images, targets = next(iter(dataloader))\n",
    "# images.shape\n",
    "# targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for images, targets in dataloader:\n",
    "    total += len(images)\n",
    "#     print(total)\n",
    "# print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    print(data)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zero 数据集读取dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from torchvision import transforms\n",
    "import webdataset as wds\n",
    "from itertools import islice\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# url = \"/home/chenweifeng/dataset/mm_data/COCO/mscoco_dataset/images/{00000..00056}.tar\"\n",
    "url = \"/home/chenweifeng/dataset/mm_data/zero23m/images/{00000..02343}.tar\"\n",
    "dataset = wds.WebDataset(url)\n",
    "print(dataset)\n",
    "for sample in islice(dataset, 0, 2):\n",
    "    for key, value in sample.items():\n",
    "        print(key, repr(value)[:50])\n",
    "    print()\n",
    "dataset = (\n",
    "    wds.WebDataset(url)\n",
    "    .shuffle(1000)\n",
    "    .decode(\"rgb\")\n",
    "    .to_tuple(\"jpg;png\", \"json\")\n",
    ")\n",
    "\n",
    "# print(dataset)\n",
    "# # # print(len(dataset))\n",
    "# total = 0\n",
    "# # for image, data in islice(dataset, 0, 5):\n",
    "# for image, data in dataset:\n",
    "#     # print(image.shape, data['caption'])\n",
    "#     total += 1\n",
    "# print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import math\n",
    "class MyIterableDataset(IterableDataset):\n",
    "    def __init__(self, start, end):\n",
    "        super(MyIterableDataset).__init__()\n",
    "        assert end > start, \"this example code only works with end >= start\"\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:  # single-process data loading, return the full iterator\n",
    "            iter_start = self.start\n",
    "            iter_end = self.end\n",
    "        else:  # in a worker process\n",
    "            # split workload\n",
    "            per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = self.start + worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, self.end)\n",
    "        return iter(range(iter_start, iter_end))\n",
    "# should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n",
    "ds = MyIterableDataset(start=3, end=7)\n",
    "\n",
    "# Single-process loading\n",
    "print(list(DataLoader(ds, num_workers=0)))\n",
    "# Mult-process loading with two worker processes\n",
    "# Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n",
    "print(list(DataLoader(ds, num_workers=2)))\n",
    "# With even more workers\n",
    "print(list(DataLoader(ds, num_workers=20)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import braceexpand\n",
    "import os\n",
    "import json\n",
    "\n",
    "def get_dataset_len(json_url):\n",
    "    shards_list = list(braceexpand.braceexpand(json_url))\n",
    "    total = 0\n",
    "    for shard in shards_list:\n",
    "        with open(shard) as f:\n",
    "            data = json.load(f)\n",
    "            # print(data)\n",
    "            total += data['successes']\n",
    "    return total\n",
    "\n",
    "def get_dataset_size(shards):\n",
    "    shards_list = list(braceexpand.braceexpand(shards))\n",
    "    dir_path = os.path.dirname(shards)\n",
    "    print(dir_path, shards_list)\n",
    "    sizes_filename = os.path.join(dir_path, 'sizes.json')\n",
    "    len_filename = os.path.join(dir_path, '__len__')\n",
    "    if os.path.exists(sizes_filename):\n",
    "        sizes = json.load(open(sizes_filename, 'r'))\n",
    "        total_size = sum([int(sizes[os.path.basename(shard)]) for shard in shards_list])\n",
    "    elif os.path.exists(len_filename):\n",
    "        # FIXME this used to be eval(open(...)) but that seemed rather unsafe\n",
    "        total_size = ast.literal_eval(open(len_filename, 'r').read())\n",
    "    else:\n",
    "        json_url = \"/home/chenweifeng/dataset/mm_data/zero23m/images/{00000..02343}_stats.json\"\n",
    "        total_size = get_dataset_len(json_url)\n",
    "        # print(total_num)\n",
    "        # total_size = None  # num samples undefined\n",
    "        # some common dataset sizes (at time of authors last download)\n",
    "        # cc3m-train: 2905954\n",
    "        # cc12m: 10968539\n",
    "        # LAION-400m: 407332084\n",
    "    num_shards = len(shards_list)\n",
    "    return total_size, num_shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size, num_shards = get_dataset_size(url)\n",
    "# json_url = \"/home/chenweifeng/dataset/mm_data/zero23m/images/{00000..02343}_stats.json\"\n",
    "# total_num = get_dataset_len(json_url)\n",
    "# print(total_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_size, num_shards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "class flickr30k_CNA(Dataset):\n",
    "    def __init__(self, img_root_path='/home/chenweifeng/dataset/mm_data/Flickr30k-CNA/flickr30k/images', \\\n",
    "                text_annot_path='/home/chenweifeng/dataset/mm_data/Flickr30k-CNA/test/flickr30k_cn_test.txt', \\\n",
    "                transform=None):\n",
    "        self.images = []\n",
    "        self.captions = []\n",
    "        self.labels = []\n",
    "        self.root = img_root_path\n",
    "        with open(text_annot_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split('\\t')\n",
    "                key, caption = line[0].split('#')[0], line[1]\n",
    "                img_path = key + '.jpg'\n",
    "                self.images.append(img_path)\n",
    "                self.captions.append(caption)\n",
    "                self.labels.append(key)\n",
    "        self.transforms = transform\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "\n",
    "        # NOTE large 模型\n",
    "        self.context_length = 77\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = str(self.images[idx])\n",
    "        image = self.transforms(Image.open( os.path.join(self.root, img_path ))) \n",
    "        text = self.tokenizer(str(self.captions[idx]), max_length=self.context_length, padding='max_length', truncation=True, return_tensors='pt')['input_ids'][0]\n",
    "        label = self.labels[idx]\n",
    "        return image, text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# directly read tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chenweifeng/dataset/mm_data/zero23m/images/00000.tar\n",
      "9113 9113\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import braceexpand\n",
    "url = \"/home/chenweifeng/dataset/mm_data/zero23m/images/{00000..02343}.tar\"\n",
    "\n",
    "images_path = []\n",
    "captions_path = []\n",
    "for each_tar in list(braceexpand.braceexpand(url)):\n",
    "    print(each_tar)\n",
    "    with tarfile.open(each_tar, \"r\") as tar:\n",
    "        for each_file in tar.getmembers():\n",
    "            # print(each_file.name)\n",
    "            if each_file.name.endswith(\".jpg\"):\n",
    "                images_path.append(each_file.name) \n",
    "            elif each_file.name.endswith(\".txt\"):\n",
    "                captions_path.append(each_file.name) \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9113 9113\n"
     ]
    }
   ],
   "source": [
    "print(len(images_path), len(captions_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import TarIO,Image\n",
    "\n",
    "fp = TarIO.TarIO('/home/chenweifeng/dataset/mm_data/zero23m/images/00000.tar', images_path[0])\n",
    "im = Image.open(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/shared_space/ccnl/mm_data/laion2B-multi-chinese-subset/image_part09/00194.tar\n",
      "8818 8818\n"
     ]
    }
   ],
   "source": [
    "# preprocess laion-zh\n",
    "import tarfile\n",
    "import braceexpand\n",
    "from glob import glob\n",
    "import os\n",
    "url = '/shared_space/ccnl/mm_data/laion2B-multi-chinese-subset/*/'\n",
    "images_path = []\n",
    "captions_path = []\n",
    "for each_tar in glob(os.path.join(url, '*.tar')):\n",
    "    print(each_tar)\n",
    "    with tarfile.open(each_tar, \"r\") as tar:\n",
    "        for each_file in tar.getmembers():\n",
    "            # print(each_file.name)\n",
    "            if each_file.name.endswith(\".jpg\"):\n",
    "                images_path.append(each_file.name) \n",
    "            elif each_file.name.endswith(\".txt\"):\n",
    "                captions_path.append(each_file.name) \n",
    "        break\n",
    "print(len(images_path), len(captions_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'001940013.txt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'001940013.jpg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for img, txt in zip(image_paths, captions_paths):\n",
    "    shutil.copy(img, '/shared_space/ccnl/mm_data/laion2B-multi-chinese-subset/images')\n",
    "    shutil.copy(txt, '/shared_space/ccnl/mm_data/laion2B-multi-chinese-subset/captions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02a2872ef89789832e0a654d6c95a175dab3d7e4133113b4cef309e372e0ba06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
